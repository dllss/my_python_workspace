# -*- coding: utf-8 -*-
"""
æ‰¹é‡è·å–Aè‚¡è‚¡ç¥¨å†å²æ•°æ®è„šæœ¬ V2ï¼ˆæ¨èï¼‰
å¤šæ•°æ®æºæ··åˆç­–ç•¥ + å¢å¼ºé”™è¯¯å¤„ç† + æ™ºèƒ½å¢é‡æ›´æ–°
æ”¯æŒï¼šbaostockï¼ˆä¼˜å…ˆï¼‰ã€akshareã€yfinance ä¸‰ä¸ªæ•°æ®æºè‡ªåŠ¨åˆ‡æ¢

ä½¿ç”¨æ–¹æ³•ï¼š
    æ–¹å¼1ï¼ˆæ¨èï¼‰ï¼š
        make run-all-v2
    
    æ–¹å¼2ï¼š
        poetry run python fetch_all_stock_hist_v2.py
    
    æ–¹å¼3ï¼š
        python fetch_all_stock_hist_v2.py  # éœ€è¦å…ˆæ¿€æ´»è™šæ‹Ÿç¯å¢ƒ

ä¸»è¦åŠŸèƒ½ï¼š
    âœ… å¤šæ•°æ®æºè‡ªåŠ¨åˆ‡æ¢ï¼ˆæé«˜ç¨³å®šæ€§ï¼‰
    âœ… æ™ºèƒ½å¢é‡æ›´æ–°ï¼ˆåªè·å–ç¼ºå¤±çš„æ•°æ®ï¼‰
    âœ… äº¤æ˜“æ—¥è¯†åˆ«ï¼ˆè‡ªåŠ¨è·³è¿‡èŠ‚å‡æ—¥/å‘¨æœ«ï¼‰
    âœ… åœç‰Œæ™ºèƒ½å¤„ç†ï¼ˆé¿å…æ— æ•ˆè¯·æ±‚ï¼‰
    âœ… æ‰¹é‡å¤„ç†ï¼ˆæ”¯æŒåˆ†æ‰¹è·å–ï¼‰
    âœ… å¤±è´¥é‡è¯•ï¼ˆæ•°æ®æºå¼‚å¸¸è‡ªåŠ¨åˆ‡æ¢ï¼‰
    âœ… åŒé‡æ•°æ®å®Œæ•´æ€§ä¿æŠ¤ï¼ˆæ—¶é—´æ£€æŸ¥ + æ ‡è®°æœºåˆ¶ï¼‰

é…ç½®å‚æ•°ï¼ˆåœ¨ config.py ä¸­ä¿®æ”¹ï¼‰ï¼š
    - START_DATE: å¼€å§‹æ—¥æœŸï¼ˆé»˜è®¤ "20000101"ï¼‰
    - END_DATE: ç»“æŸæ—¥æœŸï¼ˆé»˜è®¤æ˜¨å¤©ï¼‰
    - ADJUST_TYPE: å¤æƒç±»å‹ï¼ˆé»˜è®¤ "qfq" å‰å¤æƒï¼‰
    - BATCH_SIZE: æ‰¹æ¬¡å¤§å°ï¼ˆ0=å…¨éƒ¨ï¼Œ>0=åˆ†æ‰¹ï¼‰
    - START_INDEX: èµ·å§‹ç´¢å¼•ï¼ˆåˆ†æ‰¹å¤„ç†æ—¶ä½¿ç”¨ï¼‰
    - UPDATE_MODE: æ›´æ–°æ¨¡å¼ï¼ˆè§ä¸‹æ–¹è¯´æ˜ï¼‰
    - DELAY_MIN/MAX: è¯·æ±‚å»¶è¿Ÿï¼ˆé¿å…é¢‘ç¹è¯·æ±‚ï¼‰

æ›´æ–°æ¨¡å¼è¯´æ˜ï¼š
    - tail: åªè¡¥å……å°¾éƒ¨æ•°æ®ï¼Œå¿½ç•¥ä¸­é—´ç¼ºå¤±ï¼ˆé»˜è®¤ï¼Œæ¨èï¼‰
      é€‚ç”¨ï¼šæ—¥å¸¸å¢é‡æ›´æ–°ï¼Œé¿å…å¯¹åœç‰Œæ—¥åå¤è¯·æ±‚
    - full: å®Œå…¨åˆ·æ–°ï¼Œè¡¥å……æ‰€æœ‰ç¼ºå¤±æ•°æ®
      é€‚ç”¨ï¼šåˆæ¬¡è·å–æˆ–æ•°æ®ä¿®å¤ï¼ˆä¼šå¯¹åœç‰Œæ—¥å‘èµ·è¯·æ±‚ï¼‰
    - head_tail: è¡¥å……å¤´å°¾ï¼Œå¿½ç•¥ä¸­é—´ç¼ºå¤±
      é€‚ç”¨ï¼šæ‰©å±•å†å²æ•°æ®èŒƒå›´

æ•°æ®æºä¼˜å…ˆçº§ï¼š
    1. Baostockï¼ˆä¼˜å…ˆï¼Œæ•°æ®è´¨é‡é«˜ï¼‰
    2. Akshareï¼ˆå¤‡ç”¨ï¼Œè¦†ç›–å…¨é¢ï¼‰
    3. YFinanceï¼ˆå¤‡ç”¨ï¼Œå›½é™…æ¥å£ï¼‰

è¾“å‡ºè¯´æ˜ï¼š
    - æ•°æ®æ–‡ä»¶ï¼šdata/CN/stock_{ä»£ç }.csv
    - å¤±è´¥åˆ—è¡¨ï¼šdata/CN/failed_stocks.csv
    - ç»Ÿè®¡ä¿¡æ¯ï¼šæ–°å¢/æ›´æ–°/è·³è¿‡/å¤±è´¥æ•°é‡
    - æ•°æ®æºä½¿ç”¨ç»Ÿè®¡

æ•°æ®å®Œæ•´æ€§ä¿æŠ¤ï¼š
    ğŸ›¡ï¸ ç¬¬ä¸€å±‚ï¼š18:30 ä¹‹å‰è‡ªåŠ¨ä½¿ç”¨æ˜¨å¤©ï¼ˆBaoStock 18:00 åæ‰å®Œæ•´æ›´æ–°ï¼‰
    ğŸ›¡ï¸ ç¬¬äºŒå±‚ï¼šä¸å®Œæ•´æ•°æ®æ ‡è®°æœºåˆ¶ï¼ˆå¼‚å¸¸æƒ…å†µä¸‹çš„å¤‡ç”¨ä¿é™©ï¼‰
    ğŸ’¡ æ¨èï¼šåœ¨ 18:30 ä¹‹åè¿è¡Œè„šæœ¬ä»¥è·å–å®Œæ•´çš„å½“æ—¥æ•°æ®

æ³¨æ„äº‹é¡¹ï¼š
    âš ï¸ ä¸­é—´ç¼ºå¤±é€šå¸¸æ˜¯ä¸ªè‚¡åœç‰Œå¯¼è‡´ï¼Œéæ•°æ®ä¸¢å¤±
    âš ï¸ é¦–æ¬¡è¿è¡Œå‰éœ€å…ˆè·å–è‚¡ç¥¨åˆ—è¡¨ï¼šmake run-list
    âš ï¸ æ‰¹é‡è·å–éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œå»ºè®®ä½¿ç”¨åˆ†æ‰¹æ¨¡å¼
    âš ï¸ æ•°æ®æºå¯èƒ½é™æµï¼Œè„šæœ¬å·²åŠ å…¥éšæœºå»¶è¿Ÿ
"""

import os
import time
import random
import logging
from typing import Dict, List, Tuple, Optional
from datetime import datetime, timedelta
import pandas as pd
from config import (
    OUTPUT_DIR,
    CN_DIR,
    START_DATE,
    END_DATE,
    ADJUST_TYPE,
    DELAY_MIN,
    DELAY_MAX,
    BATCH_SIZE,
    START_INDEX,
    UPDATE_MODE,
    PREFERRED_SOURCE,
)
from fetchers import MultiSourceFetcher
from utils import (
    has_trading_day,
    get_missing_date_range,
    get_safe_end_date,
    MetadataManager,
    save_dataframe,
    merge_and_save_data
)

# ========== æ—¥å¿—é…ç½® ==========
logging.basicConfig(
    level=logging.DEBUG,  # DEBUG: æ˜¾ç¤ºè¯¦ç»†è°ƒè¯•ä¿¡æ¯ | INFO: åªæ˜¾ç¤ºå…³é”®ä¿¡æ¯ï¼ˆæ¨èï¼‰
    format='%(message)s'  # ç®€åŒ–æ ¼å¼ï¼Œåªæ˜¾ç¤ºæ¶ˆæ¯
)
logger = logging.getLogger(__name__)

# ========== åˆå§‹åŒ– ==========
cn_dir = os.path.join(OUTPUT_DIR, CN_DIR)
stock_list_file = os.path.join(cn_dir, "stock_list.csv")

# ========== æ£€æŸ¥å¹¶è°ƒæ•´ç»“æŸæ—¥æœŸï¼ˆ18:30ä¹‹å‰å¼ºåˆ¶ä½¿ç”¨æ˜¨å¤©ï¼‰ ==========
SAFE_END_DATE, date_adjusted = get_safe_end_date(END_DATE)

# ========== å¸‚åœºçŠ¶æ€æ£€æŸ¥å·²ç”± get_safe_end_date å®Œæˆ ==========
# 18:30 ä¹‹å‰å·²è‡ªåŠ¨è°ƒæ•´ä¸ºæ˜¨å¤©ï¼Œæ— éœ€é¢å¤–æ£€æŸ¥

# ========== è¯»å–è‚¡ç¥¨åˆ—è¡¨ ==========
logger.info(f"æ­£åœ¨è¯»å–è‚¡ç¥¨åˆ—è¡¨: {stock_list_file}")
logger.info(f"ç›®æ ‡æ—¥æœŸèŒƒå›´: {START_DATE} ~ {SAFE_END_DATE}")

try:
    stock_list = pd.read_csv(stock_list_file, dtype={'code': str})
    stock_list['code'] = stock_list['code'].str.zfill(6)
    total_stock_length = len(stock_list)
except FileNotFoundError:
    logger.error(f"é”™è¯¯: è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {stock_list_file}")
    logger.error("è¯·å…ˆè¿è¡Œ 'make run-list' æˆ– 'poetry run python fetch_stock_list.py' è·å–è‚¡ç¥¨åˆ—è¡¨")
    exit(1)
except Exception as e:
    logger.error(f"è¯»å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
    exit(1)

# ========== åˆ†æ‰¹å¤„ç† ==========
if BATCH_SIZE > 0:
    end_index = min(START_INDEX + BATCH_SIZE, total_stock_length)
    stock_list = stock_list[START_INDEX:end_index]
    logger.info(f"åˆ†æ‰¹æ¨¡å¼: å¤„ç†ç¬¬ {START_INDEX + 1} åˆ°ç¬¬ {end_index} åªè‚¡ç¥¨ (å…± {len(stock_list)} åª)")
    logger.info(f"æ€»è¿›åº¦: {end_index}/{total_stock_length} ({end_index/total_stock_length*100:.1f}%)\n")
else:
    logger.info(f"å…¨é‡æ¨¡å¼: å…± {total_stock_length} åªè‚¡ç¥¨\n")

# ========== æ‰¹é‡è·å–æ•°æ® ==========
success_count = 0
fail_count = 0
skip_count = 0
update_count = 0
failed_stocks = []
fetch_times: List[float] = []  # è®°å½•æ¯åªè‚¡ç¥¨çš„è·å–è€—æ—¶

# è®°å½•å¼€å§‹æ—¶é—´
start_time = time.time()

# åˆå§‹åŒ–å…ƒæ•°æ®ç®¡ç†å™¨
metadata_mgr = MetadataManager(cn_dir)
logger.debug(f"å…ƒæ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–: {metadata_mgr.get_stats()}")

# ä½¿ç”¨å¤šæ•°æ®æºç®¡ç†å™¨
logger.info(f"ä¼˜å…ˆæ•°æ®æº: {PREFERRED_SOURCE}")
with MultiSourceFetcher(preferred_source=PREFERRED_SOURCE) as multi_fetcher:
    for idx, (index, row) in enumerate(stock_list.iterrows(), 1):
        stock_code = row['code']
        stock_name = row['name']
        output_file = os.path.join(cn_dir, f"stock_{stock_code}.csv")
        
        # è®°å½•å•åªè‚¡ç¥¨å¼€å§‹æ—¶é—´
        stock_start_time = time.time()
        
        if BATCH_SIZE > 0:
            print(f"[{idx}/{len(stock_list)}] {stock_code} {stock_name} ", end="")
        else:
            print(f"[{index + 1}/{total_stock_length}] {stock_code} {stock_name} ", end="")
        
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆä½¿ç”¨å…ƒæ•°æ®åŠ é€Ÿï¼‰
            need_update, fetch_start, fetch_end, need_full_refresh, missing_dates = get_missing_date_range(
                existing_file=output_file,
                start_date=START_DATE,
                end_date=SAFE_END_DATE,
                update_mode=UPDATE_MODE,
                metadata_manager=metadata_mgr
            )
            
            if not need_update:
                stock_elapsed = time.time() - stock_start_time
                print(f"â­ï¸  å·²æ˜¯æœ€æ–° ({stock_elapsed:.2f}s)")
                skip_count += 1
                # ä¸éœ€è¦æ›´æ–°å…ƒæ•°æ®ï¼Œå› ä¸ºå…ƒæ•°æ®å·²ç»åœ¨ä¹‹å‰çš„è¿è¡Œä¸­æ­£ç¡®è®¾ç½®
                # å¦‚æœè¿™é‡Œé‡æ–°è¯»å–CSVæ›´æ–°å…ƒæ•°æ®ï¼Œä¼šå¯¼è‡´åœç‰ŒæœŸé—´çš„æ—¥æœŸè¢«é”™è¯¯è¦†ç›–
                continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰äº¤æ˜“æ—¥ï¼ˆå‘¨æœ«æˆ–èŠ‚å‡æ—¥è·³è¿‡ï¼‰
            if not has_trading_day(start_date=fetch_start, end_date=fetch_end):
                stock_elapsed = time.time() - stock_start_time
                print(f"è·å– {fetch_start}~{fetch_end}... â­ï¸  éäº¤æ˜“æ—¥ï¼Œè·³è¿‡ ({stock_elapsed:.2f}s)")
                skip_count += 1
                continue
            
            # å¤šæ•°æ®æºè·å–
            if need_full_refresh:
                print(f"æ£€æµ‹åˆ°ä¸­é—´ç¼ºå¤±ï¼Œé‡æ–°è·å– {fetch_start}~{fetch_end}...", end=" ")
                # æ‰“å°ç¼ºå¤±çš„æ—¥æœŸåˆ—è¡¨
                if missing_dates:
                    print()  # æ¢è¡Œ
                    print(f"\tç¼ºå¤±æ—¥æœŸ: {', '.join(missing_dates)}")
                    print(f"\tæ­£åœ¨è·å–...", end=" ")
            else:
                print(f"è·å– {fetch_start}~{fetch_end}...", end=" ")
            
            result = multi_fetcher.fetch(
                stock_code=stock_code,
                stock_name=stock_name,
                start_date=fetch_start,
                end_date=fetch_end,
                adjust_type=ADJUST_TYPE
            )
            
            if result.data is None:
                stock_elapsed = time.time() - stock_start_time
                if result.source == "no_data":
                    # æ•°æ®æºæ­£å¸¸ä½†æ— æ•°æ®ï¼ˆèŠ‚å‡æ—¥/åœç‰Œ/æœªä¸Šå¸‚ç­‰ï¼‰
                    print(f"â­ï¸  æ— æ•°æ®ï¼ˆèŠ‚å‡æ—¥/åœç‰Œï¼‰ ({stock_elapsed:.2f}s)")
                    skip_count += 1
                    # æ›´æ–°å…ƒæ•°æ®ï¼Œé¿å…é‡å¤æ‹‰å–
                    metadata_mgr.update_last_date(stock_code, fetch_end)
                else:
                    # æœ‰æ•°æ®æºæŠ¥é”™ï¼ŒçœŸæ­£çš„å¤±è´¥
                    print(f"âŒ æ‰€æœ‰æ•°æ®æºå‡å¤±è´¥ ({stock_elapsed:.2f}s)")
                    fail_count += 1
                    failed_stocks.append({"code": stock_code, "name": stock_name, "reason": "æ‰€æœ‰æ•°æ®æºå‡å¤±è´¥"})
                    # ä¸æ›´æ–°å…ƒæ•°æ®ï¼Œä¸‹æ¬¡éœ€è¦é‡è¯•
                continue
            
            # è·å–æ•°æ®å’Œæ•°æ®æº
            df_new = result.data
            source = result.source
            
            # åˆå¹¶å¹¶ä¿å­˜æ•°æ®ï¼ˆåŒæ—¶æ›´æ–°å…ƒæ•°æ®ï¼‰
            # ä¿ç•™å†å²åç§°ç­–ç•¥ï¼šä¸ä¿®æ”¹å†å²æ•°æ®ï¼Œæ–°æ•°æ®ä½¿ç”¨æœ€æ–°åç§°ï¼Œå¯è®°å½•åç§°å˜åŒ–å†å²
            # ä¼ é€’ fetch_end ä½œä¸ºå…ƒæ•°æ®æ›´æ–°çš„æ—¥æœŸï¼Œé¿å…å› åœç‰Œå¯¼è‡´é‡å¤æ‹‰å–
            is_update, new_count, removed_count = merge_and_save_data(
                df_new, output_file, stock_code, need_full_refresh, metadata_mgr, fetch_end
            )
            
            # è®¡ç®—è€—æ—¶
            stock_elapsed = time.time() - stock_start_time
            fetch_times.append(stock_elapsed)
            
            # æ„å»ºè¾“å‡ºä¿¡æ¯
            status = 'âœ… ' + ('åˆ·æ–°' if need_full_refresh else 'æ›´æ–°' if is_update else 'æ–°å¢')
            data_info = f"(+{new_count} æ¡"
            if removed_count > 0:
                data_info += f", è¿‡æ»¤{removed_count}æ¡åœç‰Œ"
            data_info += f") [{source}] ({stock_elapsed:.2f}s)"
            
            print(f"{status} {data_info}")
            
            if is_update:
                update_count += 1
            else:
                success_count += 1
            
            # éšæœºå»¶è¿Ÿ
            delay = random.uniform(DELAY_MIN, DELAY_MAX)
            logger.info(f"â¸ï¸  å»¶è¿Ÿ {delay:.2f}s åç»§ç»­...")
            time.sleep(delay)
            
        except KeyboardInterrupt:
            logger.warning("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜å·²å¤„ç†çš„æ•°æ®...")
            break
        except FileNotFoundError as e:
            logger.error(f"âŒ æ–‡ä»¶é”™è¯¯: {str(e)}")
            fail_count += 1
            failed_stocks.append({"code": stock_code, "name": stock_name, "reason": f"æ–‡ä»¶é”™è¯¯: {str(e)}"})
        except pd.errors.EmptyDataError as e:
            logger.error(f"âŒ æ•°æ®ä¸ºç©º: {str(e)}")
            fail_count += 1
            failed_stocks.append({"code": stock_code, "name": stock_name, "reason": f"æ•°æ®ä¸ºç©º: {str(e)}"})
        except Exception as e:
            logger.error(f"âŒ å¼‚å¸¸: {str(e)}")
            fail_count += 1
            failed_stocks.append({"code": stock_code, "name": stock_name, "reason": str(e)})
    
    # è·å–æ•°æ®æºä½¿ç”¨ç»Ÿè®¡
    source_stats = multi_fetcher.get_stats()

# è®¡ç®—æ€»è€—æ—¶
total_elapsed = time.time() - start_time

# ========== ç»Ÿè®¡ç»“æœ ==========
logger.info("\n" + "=" * 60)
logger.info("æ‰¹é‡è·å–å®Œæˆ")
logger.info("=" * 60)
if BATCH_SIZE > 0:
    logger.info(f"æœ¬æ‰¹å¤„ç†: {len(stock_list)} åªè‚¡ç¥¨")
    logger.info(f"æ€»è¿›åº¦: {end_index}/{total_stock_length} ({end_index/total_stock_length*100:.1f}%)")
else:
    logger.info(f"æ€»è®¡: {total_stock_length} åªè‚¡ç¥¨")
logger.info(f"æ–°å¢: {success_count} åª")
logger.info(f"æ›´æ–°: {update_count} åª")
logger.info(f"è·³è¿‡: {skip_count} åªï¼ˆæ•°æ®å·²æ˜¯æœ€æ–°ï¼‰")
logger.info(f"å¤±è´¥: {fail_count} åª")

logger.info(f"\næ•°æ®æºä½¿ç”¨ç»Ÿè®¡:")
logger.info(f"  akshare: {source_stats['akshare']} åª")
logger.info(f"  baostock: {source_stats['baostock']} åª")
logger.info(f"  yfinance: {source_stats['yfinance']} åª")

# è€—æ—¶ç»Ÿè®¡
logger.info(f"\nâ±ï¸  è€—æ—¶ç»Ÿè®¡:")
logger.info(f"  æ€»è€—æ—¶: {total_elapsed:.2f}s ({total_elapsed/60:.2f}min)")
if fetch_times:
    avg_time = sum(fetch_times) / len(fetch_times)
    max_time = max(fetch_times)
    min_time = min(fetch_times)
    logger.info(f"  å¹³å‡è€—æ—¶: {avg_time:.2f}s/åª")
    logger.info(f"  æœ€å¿«: {min_time:.2f}s")
    logger.info(f"  æœ€æ…¢: {max_time:.2f}s")
    logger.info(f"  å®é™…è·å–: {len(fetch_times)} åª")
if len(stock_list) > 0:
    logger.info(f"  å¹³å‡é€Ÿåº¦: {len(stock_list)/total_elapsed:.2f} åª/ç§’")

if BATCH_SIZE > 0 and end_index < total_stock_length:
    next_start = end_index
    logger.info(f"\nğŸ’¡ æç¤º: è¿˜æœ‰ {total_stock_length - end_index} åªè‚¡ç¥¨æœªå¤„ç†")

if failed_stocks:
    logger.info(f"\nå¤±è´¥åˆ—è¡¨:")
    for stock in failed_stocks:
        logger.info(f"  - {stock['code']} {stock['name']}: {stock['reason']}")
    
    # ä¿å­˜å¤±è´¥åˆ—è¡¨
    try:
        failed_df = pd.DataFrame(failed_stocks)
        failed_file = os.path.join(cn_dir, "failed_stocks.csv")
        failed_df.to_csv(failed_file, index=False, encoding="utf-8-sig")
        logger.info(f"\nå¤±è´¥åˆ—è¡¨å·²ä¿å­˜åˆ°: {failed_file}")
    except Exception as e:
        logger.error(f"ä¿å­˜å¤±è´¥åˆ—è¡¨æ—¶å‡ºé”™: {e}")

# ========== è„šæœ¬æ‰§è¡Œå®Œæˆ ==========
# 18:30 æ—¶é—´æ£€æŸ¥å·²ç¡®ä¿æ•°æ®å®Œæ•´æ€§ï¼Œæ— éœ€é¢å¤–æé†’
